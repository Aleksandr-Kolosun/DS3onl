{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Dask: когда не справляется Pandas</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используйте NumPy \n",
    "- Если ваши данные удобно помещаются в ОЗУ и вы не ограничены в производительности, то использование NumPy может быть правильным выбором. Dask добавляет еще один уровень сложности, который может помешать. Если вы просто ищете ускорение, а не масштабируемость, вы можете рассмотреть такой проект, как Numba. [Подробнее про лучшие практики](https://docs.dask.org/en/stable/array-best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный тьюториал содержит краткий обзор библиотеки Dask и более подробное описание возможностей dask.dataframe.\n",
    "<br>\n",
    "При подготовке тьюториала использовались данные [2017 NYC Taxi Rides](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое Dask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.dask.org/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask - библиотека Python для параллельных вычислений.** Работает как на одной машине, максимально используя доступные вычислительные ресурсы, так и на кластере до 1000 ядер. Однако, как заметил разработчик Dask Matthew Rocklin: \"Медианный размер кластера Dask - 1 компьютер\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Компоненты Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Big data collections** - параллельные \"ленивые\" обёртки для датафреймов Pandas, массивов NumPy и итераторов для работы с данными, размер которых превышает объем памяти.\n",
    "2. **Dynamic task scheduling** - планировщик задач, оптимизированный для вычислений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://docs.dask.org/en/stable/_images/dask-overview.svg\" height=\"30%\" widht=\"30%\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В отдельные проекты выделены:**\n",
    "1. [Dask-ML](https://ml.dask.org/) - оптимизированные алгоритмы sklearn, dask-xgboost (!), dask-tensorflow (!) и про \"это ваше машинное обучение\" в масштабах кластера. \n",
    "2. [Dask-distributed](https://distributed.readthedocs.io/en/latest/) - про dask на распределенном кластере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### К теме тьюториала: использование dask.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask.dataframe - это распределенный pandas.DataFrame. Если Dask.dataframe не помещается в память, то в RAM последовательно подгружаются соответствующие объему памяти части, а \"излишки\" хранятся на диске."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какие проблемы pandas решает dask.dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема №1**: данные должны помещаться в память\n",
    "<br>\n",
    "**Решение Dask**: работает с данными, которые не умещаются в память\n",
    "<br><br>\n",
    "**Проблема №2**: вычисления в 1 поток\n",
    "<br>\n",
    "**Решение Dask**: автоматическая параллелизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" height=\"20%\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерфейс dask.dataframe аналогичен pandas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pandas                                 #dask\n",
    "import pandas as pd                     import dask.dataframe as dd\n",
    "df = pd.read_csv('2015-01-01.csv')      df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby(df.user_id).value.mean()     df.groupby(df.user_id).value.mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Имеется 2 файла:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считывать будем только 4 столбца:** `VendorID`, `tpep_pickup_datetime`, `passenger_count`, `total_amount`\n",
    "<br>\n",
    "Этих данных достаточно для демонстрации возможностей Dask <s>да и комп у меня слабый</s> :trollface:\n",
    "<br><br>\n",
    "Параметры для считывания файлов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(header=0, \n",
    "              usecols = [0, 1, 3, 16],\n",
    "              dtype = {'1': 'datetime64'},\n",
    "              #небольшой костыль для корректного считывания данных\n",
    "              converters = {'Passenger_count': (lambda x: round(float(x), 0) // 1 if (x != 'NaN' or len(x) <= 5) else 0), \n",
    "                            'Total_amount': (lambda x: float(x) if (x != 'NaN' or len(x) <= 5) else 0)}\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Читаем 1 файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://data.cityofnewyork.us/Transportation/2017-Yellow-Taxi-Trip-Data/biws-g3hs данные тут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andru\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\magics\\execution.py:1316: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code, glob, local_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.2 s\n",
      "Wall time: 13.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16200443</th>\n",
       "      <td>2</td>\n",
       "      <td>05/05{</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200444</th>\n",
       "      <td>\"error\" : true</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200445</th>\n",
       "      <td>\"message\" : \"Internal error\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200446</th>\n",
       "      <td>\"status\" : 500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200447</th>\n",
       "      <td>}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                VendorID tpep_pickup_datetime  \\\n",
       "16200443                               2               05/05{   \n",
       "16200444                  \"error\" : true                  NaN   \n",
       "16200445    \"message\" : \"Internal error\"                  NaN   \n",
       "16200446                  \"status\" : 500                  NaN   \n",
       "16200447                               }                  NaN   \n",
       "\n",
       "          passenger_count  total_amount  \n",
       "16200443              NaN           NaN  \n",
       "16200444              NaN           NaN  \n",
       "16200445              NaN           NaN  \n",
       "16200446              NaN           NaN  \n",
       "16200447              NaN           NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pandas_df = pd.read_csv('2017_Yellow_Taxi_Trip_Data.csv', **params)\n",
    "pandas_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andru\\AppData\\Local\\Temp\\ipykernel_16488\\3372663491.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  pandas_df.passenger_coun = pandas_df.passenger_count.dropna()\n"
     ]
    }
   ],
   "source": [
    "pandas_df.passenger_coun = pandas_df.passenger_count.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6206014860210922"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.passenger_count.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 625 ms\n",
      "Wall time: 3.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>03/17/2017 12:19:39 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>03/17/2017 12:19:39 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>03/17/2017 12:19:39 AM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>03/17/2017 12:19:39 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>03/17/2017 12:19:40 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID    tpep_pickup_datetime  passenger_count  total_amount\n",
       "0       2.0  03/17/2017 12:19:39 AM              1.0         12.30\n",
       "1       2.0  03/17/2017 12:19:39 AM              1.0          6.62\n",
       "2       2.0  03/17/2017 12:19:39 AM              5.0          5.30\n",
       "3       2.0  03/17/2017 12:19:39 AM              1.0         11.76\n",
       "4       2.0  03/17/2017 12:19:40 AM              1.0         17.76"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dask_df = dd.read_csv('2017_Yellow_Taxi_Trip_Data.csv', **params, assume_missing=True)\n",
    "dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andru\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:125: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  df = pandas_read_text(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6206014860210922"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_df.passenger_count.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Dask справился значительно быстрее, потому что pandas сначала считывает файл и выводит первые 5, а dask считывает 5 строк и сразу их выводит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, когда файл помещается в оперативную память, pandas с уже загруженными данными серьезно превосходит dask, работающий по \"ленивому\" принципу - вычисления и обработка данных происходят непосредственно при вызове метода. Реализация \"ленивого\" подхода, в принципе, характерна для ресурсоемких операций. Особенно, когда дело касается \"настоящей бигдаты\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следим за использованием памяти, удаляем ненужные объекты, собираем мусор:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del pandas_df, dask_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем 2 файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 984 ms, sys: 132 ms, total: 1.12 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# dask_df2 = dd.read_csv('data/*.csv', **params)\n",
    "# dask_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 2.41 s, total: 28.3 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# pandas_df2 = pd.concat([pd.read_csv(fn, **params) for fn in glob.glob('data/*.csv')])\n",
    "# pandas_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Учитывая, что загружаемые файлы примерно одинакового размера (~800 Mb), видим, что время обработки увеличилось линейно. Очевидно, если грузить реально большой файл(-ы), pandas рано или поздно упрётся в лимит RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK - памяти хватает, но считает медленно..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для устранения этого неудобства можно просто преобразовать pandas.DataFrame в dask.datafram и считать всеми имеющимися ядрами. Автоматически, без дополнительного кода и настроек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Используем pandas_df2 из предыдущего примера:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.06 s\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dask_df3 = dd.from_pandas(pandas_df, npartitions=10, chunksize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas'овский датафрейм просто переопределим для нумерации датафреймов:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pandas_df3 = pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Уборка:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1945"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del pandas_df2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed-test: dask VS. pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров, наглядно демонстрирующих: с помощью dask можно значительно ускорить обработку данных.\n",
    "<br><br>\n",
    "Обратите внимание на метод `compute()` при обработке dask датафрейма - это как раз команда \"посчитать\". Без нее \"ленивый\" dask лишь определит, что нужно будет сделать непосредственно при запросе пользователя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 18 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "538482.68"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pandas_df3['total_amount'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 14 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "538482.68"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dask_df3['total_amount'].max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 188 ms\n",
      "Wall time: 181 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    11647707\n",
       "2.0     2328532\n",
       "5.0      771051\n",
       "3.0      660530\n",
       "6.0      474534\n",
       "4.0      316162\n",
       "0.0        1801\n",
       "8.0          50\n",
       "7.0          45\n",
       "9.0          31\n",
       "Name: passenger_count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pandas_df3['passenger_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 243 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    11647707\n",
       "2.0     2328532\n",
       "5.0      771051\n",
       "3.0      660530\n",
       "6.0      474534\n",
       "4.0      316162\n",
       "0.0        1801\n",
       "8.0          50\n",
       "7.0          45\n",
       "9.0          31\n",
       "Name: passenger_count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dask_df3['passenger_count'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. groupby() - sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 969 ms\n",
      "Wall time: 963 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VendorID\n",
       "1                                  9259919.0\n",
       "2                                 16973998.0\n",
       "  \"error\" : true                         0.0\n",
       "  \"message\" : \"Internal error\"           0.0\n",
       "  \"status\" : 500                         0.0\n",
       "1                                     7459.0\n",
       "2                                    13086.0\n",
       "}                                        0.0\n",
       "Name: passenger_count, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pandas_df3.groupby(by='VendorID')['passenger_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 969 ms\n",
      "Wall time: 774 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VendorID\n",
       "1                                  9259919.0\n",
       "2                                 16973998.0\n",
       "  \"error\" : true                         0.0\n",
       "  \"message\" : \"Internal error\"           0.0\n",
       "  \"status\" : 500                         0.0\n",
       "1                                     7459.0\n",
       "2                                    13086.0\n",
       "}                                        0.0\n",
       "Name: passenger_count, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dask_df3.groupby(by='VendorID')['passenger_count'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Очевидно, dask, автоматически используя доступные ресурсы, работает быстрее pandas даже при простых операциях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask.dataframe API является частью Pandas API, но не является его полной копией - следует знать о некоторых ограничениях, например:\n",
    "1. Операции, связанные с индексированием (новый индекс) несортированных данных, затратны с вычислительной точки зрения\n",
    "2. Посторочная обработка работает медленно как в pandas, так и в dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask - простой и мощный инструмент для чтения больших файлов и обработки данных. Использвание dask.dataframe позволяет максимально использовать ресурсы компьютера без дополнительного кода и настроек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dask.dataframe <font color=\"green\">рекомендуется</font> использовать, когда:**\n",
    "1. Необходимо считать и обработать данные, не помещающиеся в память\n",
    "2. Конфигурация компьютера позволяет задействовать в вычислениях несколько ядер процессора\n",
    "3. Распределенная обработка больших датасетов с помощью стандартных инструмнтов Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использование dask.dataframe <font color=\"red\">не рекомендуется</font>, когда:**\n",
    "1. Данные помещаются в память - pandas может справляться быстрее\n",
    "2. Данные не соответствуют табличному формату pandas\n",
    "3. Необходимо использование функционала, не реализованного в dask.dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Источники информации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Очень крутая и подробная [документация Dask](http://dask.pydata.org/en/latest/docs.html)\n",
    "2. Презентация Matthew Rocklin, разработчика Dask - [Dask: Parallel Programming in Python](http://matthewrocklin.com/slides/dask-short.html)\n",
    "3. Материалы митапа \"Машинное обучение в Новосибирске\" - [Дмитрий Колодезев о Dask](https://www.youtube.com/watch?time_continue=193&v=emd2NOC05es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(header=0, \n",
    "              usecols = [0, 1, 3, 16],\n",
    "              dtype = {'1': 'datetime64','VendorID': 'int64'},\n",
    "              #небольшой костыль для корректного считывания данных\n",
    "              converters = {'Passenger_count': (lambda x: round(float(x), 0) // 1 if (x != 'NaN' or len(x) <= 5) else 0), \n",
    "                            'Total_amount': (lambda x: float(x) if (x != 'NaN' or len(x) <= 5) else 0),\n",
    "                            'VendorID': (lambda x: int(x) if (x != 'NaN' or len(x) <= 5) else 0)}\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\biaspaltsau_aa\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Both a converter and dtype were specified for column VendorID - only the converter will be used\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID    tpep_pickup_datetime  passenger_count  total_amount\n",
      "0         2  03/17/2017 12:19:39 AM                1         12.30\n",
      "1         2  03/17/2017 12:19:39 AM                1          6.62\n",
      "2         2  03/17/2017 12:19:39 AM                5          5.30\n",
      "3         2  03/17/2017 12:19:39 AM                1         11.76\n",
      "4         2  03/17/2017 12:19:40 AM                1         17.76\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Dask dataframe\n",
    "ddf = dd.read_csv(\"2017_Yellow_Taxi_Trip_Data.csv\",**params)\n",
    "print(ddf.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 4 entries, VendorID to total_amount\n",
      "dtypes: object(1), float64(1), int64(2)"
     ]
    }
   ],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 4 entries, VendorID to total_amount\n",
      "dtypes: object(1), float64(1), int64(2)"
     ]
    }
   ],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5114a98448da61d3c8717f10dbe3cc222d3772cc8c6832b6d605cb65183e3f6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
