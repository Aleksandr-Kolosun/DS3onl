{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "993c4538",
   "metadata": {},
   "source": [
    "Мышковец С.А., v.1 1.05.2023\n",
    "\n",
    "**РЕШЕНИЕ ЗАДАЧИ**:\n",
    "\n",
    "Сравнить существующие решения по саммаризации тестов.\n",
    "\n",
    "1. Изучить метрики, используемые для оценки качества саммаризации.\n",
    "2. Привести примеры саммаризации различными моделями трех видов текстов (статьи, отзыва с amazon, диалога).\n",
    "3. Сравнить метрики.\n",
    "\n",
    "---\n",
    "---\n",
    "**ВЫВОД**:\n",
    "\n",
    "1. При оценке саммаризации  сравниваются предикты с кратким содержанием, созданным человеком. Не совсем показательный критерий для абстрактных кратких содержаний. \n",
    "2. Модели с низкими результатами могут выдавать достаточно качественные обобщения.\n",
    "3. На саммаризации диалогов модель на основе **TextRank** показала результаты сравнимые с результатами модели **philschmid/bart-large-cnn-samsum**, хотя обобщение первой можели смысла не имело, а вторая справилась с заданием отлично.\n",
    "\n",
    "---\n",
    "**Extractive summarazation models**:\n",
    "\n",
    "1. Модель на основе **TextRank** неплохо справилась с обощением статьи и отзыва, диалог резюмировать не удалось (результат обощения не имеет смысла).\n",
    "2. На фоне моделей абстрактного обобщения смотрится слабо, так как не генерируют текст, а только извлекают самые значимые предложения.\n",
    "\n",
    "---\n",
    "**Abstractive summarazation models**:\n",
    "\n",
    "1. Все модели абстрактной саммаризации хорошо справляются с обобщением статьи и отзыва, с диалогами у большинства моделей проблемы.\n",
    "\n",
    "2. Только модель **philschmid/bart-large-cnn-samsum** , обученная на датасете samsum, и **chart-gpt4** смогли справиться с саммаризацией диалога.\n",
    "\n",
    "3. Модель **mT5 Model** from Transformers была обучена на отзывах и хорошо справляется с их саммаризацией. Но несмотря на это, если прогнать через модель большое кол-во отзывов, есть сильные смысловые и эмоциональные отклонения.\n",
    "\n",
    "4. Модель **XLNet based Transformers** показала низкие метрики, но при саммаризации новостной статьи выдала хоть и очень сжатую, но точную выдержку.\n",
    "\n",
    "5. Лучше всех с саммаризацией справился **chart-gpt4**. Модель отлично справляется при любых условиях по длине краткого содержания, не теряя смысла и эмоциональной окраски.\n",
    "\n",
    "---\n",
    "**Результаты самммаризации для новостной СТАТЬИ:**\n",
    "\n",
    "| Summarization type| EXTRACTIVE | ABSTRACTIVE | ABSTRACTIVE | ABSTRACTIVE | ABSTRACTIVE  | ABSTRACTIVE | ABSTRACTIVE\n",
    "| :---        |    :----:   |    :----: |   :----:   |    :----:   |    :----:   |    :----:   | :----:   | \n",
    "| **Metrics**| **TextRank** | **philschmid/bart-large-cnn-samsum** | **Google’s T5 Model from Transformers** | **mT5 Model from Transformers** | **bert-extractive-summarizer GPT2 based Transformers**  |**bert-extractive-summarizer XLNet based Transformers**  | **chart-gpt4** \n",
    "| origin lenfth | 258 | 258 |258 | 258 | 258 | 258 | 258 |\n",
    "| summary lenfth |  59 | 54 |42 | 2 | 54 |72 |91|\n",
    "| rouge1 |  0.38 | 0.38 | 0.41 | 0.03 | 0.46 |0.08 |0.34|\n",
    "| rouge2 | 0.14 | 0.13 | 0.17 | 0.0 | 0.27 | 0.0 | 0.14|\n",
    "| rougeL | 0.19 | 0.22 | 0.32 | 0.03 | 0.37| 0.08| 0.30|\n",
    "| rougeLsum  | 0.33 | 0.30 | 0.34 | 0.03 | 0.33 | 0.08| 0.30 |\n",
    "\n",
    "**Все показатели качества самммаризации всех рассмотренных моделей для ОТЗЫВОВ равны 0. Отзывы сами по себе короткие и требуют еще более сжатого вывода краткого содержания.**\n",
    "\n",
    "**Результаты самммаризации для ДИАЛОГА:**\n",
    "\n",
    "| Summarization type| EXTRACTIVE | ABSTRACTIVE | ABSTRACTIVE | ABSTRACTIVE | ABSTRACTIVE  | ABSTRACTIVE | ABSTRACTIVE\n",
    "| :---        |    :----:   |    :----: |   :----:   |    :----:   |    :----:   |    :----:   | :----:   | \n",
    "| **Metrics**| **TextRank** | **philschmid/bart-large-cnn-samsum** | **Google’s T5 Model from Transformers** | **mT5 Model from Transformers** | **bert-extractive-summarizer GPT2 based Transformers**  |**bert-extractive-summarizer XLNet based Transformers**  | **chart-gpt4** \n",
    "| origin lenfth | 107 | 107 | 107 | 107 | 107 | 107 | 107 |\n",
    "| summary lenfth |  21 | 44 | 32 | 2 | 4 | 23 | 61|\n",
    "| rouge1 |  0.3 | 0.33 | 0.12 | 0.10 | 0.0 |0.0 |0.28 |\n",
    "| rouge2 | 0.05 | 0.06 | 0.04 | 0.0 | 0.0 | 0.0 | 0.10 |\n",
    "| rougeL | 0.3 | 0.2 | 0.12 | 0.10 | 0.0| 0.0 | 0.23 |\n",
    "| rougeLsum  | 0.3 | 0.2 | 0.12 | 0.10 | 0.0 | 0.0| 0.23 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bd0d740",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e939fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7665f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "378e4956",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e04a6d",
   "metadata": {},
   "source": [
    "# Metric Card for ROUGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c2509cb",
   "metadata": {},
   "source": [
    "Для оценки качества массаризации используется метрика rouge (or Recall-Oriented Understudy for Gisting Evaluation). Эта метрика представляет собой набор показателей и программный пакет, используемый для оценки программного обеспечения для автоматического суммирования и машинного перевода при обработке естественного языка. Метрики сравнивают автоматически созданное краткое содержание или перевод со справочной информацией или набором ссылок (созданным человеком) кратким содержанием или переводом.\n",
    "\n",
    "ROUGE нечувствителен к регистру, что означает, что буквы верхнего регистра обрабатываются так же, как буквы нижнего регистра.\n",
    "\n",
    "Эта метрика является оберткой  в Google Research reimplementation of ROUGE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dfd149d",
   "metadata": {},
   "source": [
    "**ROUGE = 2 * PRESICION * RECALL / PRECISION + RECALL**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc1fe9e",
   "metadata": {},
   "source": [
    "**Виды метрики rouge:**\n",
    "\n",
    "\"rouge1\": unigram (1-gram) based scoring\n",
    "\n",
    "\"rouge2\": bigram (2-gram) based scoring\n",
    "\n",
    "\"rougeL\": Longest common subsequence based scoring.\n",
    "\n",
    "\"rougeLSum\": splits text using \"\\n\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac7b9658",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/evaluate-metric/rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate\n",
    "# !pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "554cd975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Пример использования метрики ROUGE\n",
    "rouge = evaluate.load('rouge')\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = rouge.compute(predictions=predictions,\n",
    "                         references=references,\n",
    "                            tokenizer=lambda x: x.split())\n",
    "print(results)\n",
    "# {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e304847e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa41c866",
   "metadata": {},
   "source": [
    "# Рассмотрим саммаризацию новостной статьи, отзыва с amazon и несложного диалога."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c857896",
   "metadata": {},
   "source": [
    "**Обозначим тексты для саммаризации и определим функции для оценки ее качества.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ff5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_bbc_news = \"\"\"Residents of a tiny Swiss village have all been evacuated because of the risk of an imminent rockslide.\n",
    "Brienz's fewer than 100 villagers were given just 48 hours to pack what they could and abandon their homes.\n",
    "Even the dairy cows were loaded up for departure after geologists warned a rockfall was imminent.\n",
    "Two million cubic metres of rock is coming loose from the mountain above, and a rockslide could obliterate the village.\n",
    "The development has raised questions about the safety of some mountain communities, as global warming changes the alpine environment.\n",
    "Brienz, in the eastern canton of Graubünden, is now empty.\n",
    "The village has been judged a geological risk for some time and is built on land that is subsiding down towards the valley, causing the church spire to lean and large cracks to appear in buildings.\n",
    "As the minutes ticked towards the deadline to leave, even Brienz's dairy cows were being taken to safety.\n",
    "The residents, some young, some old, families, farmers and professional couples, had two days to abandon their homes.\n",
    "They were asked earlier this week to evacuate the village by Friday evening.\n",
    "Switzerland's Alpine regions are especially sensitive to global warming - as the permafrost high in the mountains begins to thaw, the rock becomes more unstable.\n",
    "This particular mountain has always been unstable, but recently the rock has been shifting faster and faster.\n",
    "Days of heavy rain could bring two million cubic metres of loosened rock crashing down the mountainside onto the village, scientists warned.\n",
    "Now the villagers must wait, in temporary accommodation, for the rock to fall - and hope it misses their homes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "186a5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_produced_summary_bbc_news = \"\"\"Residents of a tiny Swiss village have all been evacuated  within 48 hours due to the risk of an imminent rockslide. \n",
    "The residents must wait and hope their homes will not be destroyed by loosened rock crashing down the mountainside. \n",
    "The safety of mountain communities has been judged a geological risk for some time and is considered to be a global warming consequence.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b680d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_article_summary(prediction):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    predictions = [prediction]\n",
    "    references = [human_produced_summary_bbc_news]\n",
    "    results = rouge.compute(predictions=predictions,\n",
    "                        references=references,\n",
    "                        # tokenizer=lambda x: x.split() to use for non-latin languages\n",
    "                            )\n",
    "    print(f'origin length:', len(ARTICLE_bbc_news.split(' ')))\n",
    "    print(f'summary length:',len(prediction.split(' ')))\n",
    "    print(prediction)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "740bfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW= \"\"\"I feel I have to write to keep others from wasting their money. \n",
    "This book seems to have been written by a 7th grader with poor grammatical skills for her age! \n",
    "As another reviewer points out, there is a misspelling on the cover, and I believe there is at least one per chapter. \n",
    "For example, it was mentioned twice that she had a \"lean\" on her house. \n",
    "I was so distracted by the poor writing and weak plot, that I decided to read with a pencil in hand to mark all of the horrible grammar and spelling. \n",
    "Please don't waste your money. I too, believe that the good reviews must have been written by the author's relatives. I will not put much faith in the reviews from now on!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c215257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_produced_summary_review = \"Awful beyond belief!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7820a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_review_summary(prediction):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    predictions = [prediction]\n",
    "    references = [human_produced_summary_review]\n",
    "    results = rouge.compute(predictions=predictions,\n",
    "                        references=references,\n",
    "                        # tokenizer=lambda x: x.split() to use for non-latin languages\n",
    "                            )\n",
    "    print(f'origin length:', len(REVIEW.split(' ')))\n",
    "    print(f'summary length:',len(prediction.split(' ')))\n",
    "    print(prediction)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a100df",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIALOGUE = \"\"\"Jane: Good morning, Doctor Rudra, how are you doing?\n",
    "Doctor Rudra: Good morning, Jane. I am doing well. And you?\n",
    "Jane: I’m great, thank you. This is my friend Leila. She is thinking about joining the hospital but she has a few questions about the administration there. Would you mind telling her about the administration, please?\n",
    "Doctor Rudra: Hello, Leila! It’s a pleasure to meet you. I’m more than happy to speak with you. Please stop by my chamber tomorrow.\n",
    "Leila: It’s a pleasure to meet you, Doctor. Thank you so much for helping us.\n",
    "Doctor Rudra: Don’t mention it. Hopefully, I will be able to help you out in this matter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0dd6fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_produced_summary_dialogue = ['Jane asked Doctor Rudra to tell her friend Leila about hospital administration before she joins this hospital.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1c56224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dialogue_summary(prediction):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    predictions = [prediction]\n",
    "    references = [human_produced_summary_dialogue]\n",
    "    results = rouge.compute(predictions=predictions,\n",
    "                        references=references,\n",
    "                        # tokenizer=lambda x: x.split() to use for non-latin languages\n",
    "                            )\n",
    "    print(f'origin length:', len(DIALOGUE.split(' ')))\n",
    "    print(f'summary length:',len(prediction.split(' ')))\n",
    "    print(prediction)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c2d13a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_test = '''Waiter: Hello, good evening. Can I start you off with some refreshing drink?\n",
    "Rana: Yes. I’ll have iced tea, please.\n",
    "Amal: And I’ll have a chocolate cold coffee.\n",
    "Waiter: Ok. Should I take your order now, or do you need a few minutes more?\n",
    "Rana: No no we are ready, you can take the order. I’ll have the corn mushroom soup to start, and the grilled chicken with mashed potatoes and peas. And, please also bring a bowl of garlic rice.\n",
    "Waiter: Sure sir. How do you want the chicken— low spicy, medium, or high on spice?\n",
    "Rana: Medium spice, please.\n",
    "Amal: And I’ll just have the beef, with bread and a salad.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3481a044",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ddcb46b",
   "metadata": {},
   "source": [
    "# **EXTRACTIVE SUMMARIZATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97031209",
   "metadata": {},
   "source": [
    "**Extractive Text Summarization Methods: TextRank, Latent Semantic Analysis.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3698810a",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41dd11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extractive = pd.DataFrame()\n",
    "df_extractive['to summarize'] = [ARTICLE_bbc_news, REVIEW, DIALOGUE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cf05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5532c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e5568b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/yypls09j61bb1cjkz85h1n7c0000gn/T/ipykernel_74131/3981306276.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sviatlanamyshkavets/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days of heavy rain could bring two million cubic metres of loosened rock crashing down the mountainside onto the village, scientists warned.\n",
      "Now the villagers must wait, in temporary accommodation, for the rock to fall - and hope it misses their homes.\n",
      "Brienz's fewer than 100 villagers were given just 48 hours to pack what they could and abandon their homes.\n"
     ]
    }
   ],
   "source": [
    "# split the the text in the articles into sentences\n",
    "sentences = []\n",
    "for s in df_extractive['to summarize'][:1]:\n",
    "  sentences.append(sent_tokenize(s))  \n",
    "\n",
    "# flatten the list\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "# Text Preprocessing\n",
    "# It is always a good practice to make your textual data noise-free as much as possible. So, let’s do some basic text cleaning.\n",
    "\n",
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "# Get rid of the stopwords (commonly used words of a language – is, am, the, of, in, etc.) present in the sentences. If you have not downloaded nltk-stopwords, then execute the following line of code:\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# Now we can import the stopwords.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Let’s define a function to remove these stopwords from our dataset.\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "# We will use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors.\n",
    "\n",
    "# Vector Representation of Sentences\n",
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "# Now, let’s create vectors for our sentences. We will first fetch vectors (each of size 100 elements) for the constituent words in a sentence and then take mean/average of those vectors to arrive at a consolidated vector for the sentence.\n",
    "\n",
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)\n",
    "\n",
    "# Similarity Matrix Preparation\n",
    "# The next step is to find similarities between the sentences, and we will use the cosine similarity approach for this challenge. Let’s create an empty similarity matrix for this task and populate it with cosine similarities of the sentences.\n",
    "\n",
    "# Let’s first define a zero matrix of dimensions (n * n).  We will initialize this matrix with cosine similarity scores of the sentences. Here, n is the number of sentences.\n",
    "\n",
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "# We will use Cosine Similarity to compute the similarity between a pair of sentences.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# And initialize the matrix with cosine similarity scores.\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "\n",
    "# Applying PageRank Algorithm\n",
    "# Before proceeding further, let’s convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings.\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# Summary Extraction\n",
    "# Finally, it’s time to extract the top N sentences based on their rankings for summary generation.\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "# Extract top 10 sentences as the summary\n",
    "for i in range(3):\n",
    "  print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5aefae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 258\n",
      "summary length: 59\n",
      "Days of heavy rain could bring two million cubic metres of loosened rock crashing down the mountainside onto the village, scientists warned.\n",
      "Now the villagers must wait, in temporary accommodation, for the rock to fall - and hope it misses their homes.\n",
      "Brienz's fewer than 100 villagers were given just 48 hours to pack what they could and abandon their homes.\n",
      "{'rouge1': 0.3870967741935484, 'rouge2': 0.14754098360655737, 'rougeL': 0.1935483870967742, 'rougeLsum': 0.3387096774193548}\n"
     ]
    }
   ],
   "source": [
    "eval_article_summary(\"\"\"Days of heavy rain could bring two million cubic metres of loosened rock crashing down the mountainside onto the village, scientists warned.\n",
    "Now the villagers must wait, in temporary accommodation, for the rock to fall - and hope it misses their homes.\n",
    "Brienz's fewer than 100 villagers were given just 48 hours to pack what they could and abandon their homes.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef70e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 129\n",
      "summary length: 38\n",
      "I too, believe that the good reviews must have been written by the author's relatives.\n",
      "I will not put much faith in the reviews from now on!\n",
      "I feel I have to write to keep others from wasting their money.\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_review_summary(\"\"\"I too, believe that the good reviews must have been written by the author's relatives.\n",
    "I will not put much faith in the reviews from now on!\n",
    "I feel I have to write to keep others from wasting their money.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dabee971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 21\n",
      "Doctor Rudra: Good morning, Jane.\n",
      "Jane: Good morning, Doctor Rudra, how are you doing?\n",
      "Would you mind telling her about the administration, please?\n",
      "{'rouge1': 0.3, 'rouge2': 0.052631578947368425, 'rougeL': 0.3, 'rougeLsum': 0.3}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary('''Doctor Rudra: Good morning, Jane.\n",
    "Jane: Good morning, Doctor Rudra, how are you doing?\n",
    "Would you mind telling her about the administration, please?''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73a041b3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ba2fd36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae09fc2",
   "metadata": {},
   "source": [
    "# **ABSTRACTIVE SUMMARIZATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0791845",
   "metadata": {},
   "source": [
    "**Метод абстрактного резюмирования хорошо работает с моделями глубокого обучения, такими как модель seq2seq, LSTM, BERT, T5 Transformers и т. д., а также с популярными пакетами Python (Spacy, NLTK и т. д.) и фреймворками (Tensorflow, Keras).**\n",
    "\n",
    "**Предобученные модели (BERT, GPT2, XLNET) считаются самыми современными для обобщения текста.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "277cec1f",
   "metadata": {},
   "source": [
    "Для эксперимента выбрана модель с https://huggingface.co/ фильтром MOST LIKES & MOST DOWNLOADS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f12aa70",
   "metadata": {},
   "source": [
    "# philschmid/bart-large-cnn-samsum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c013b84b",
   "metadata": {},
   "source": [
    "**Описание модели**\n",
    "BART представляет собой модель преобразователя encoder-decoder (seq2seq) с двунаправленным (подобным BERT) encoder и авторегрессионным (подобным GPT) decoder. BART предварительно обучается путем (1) искажения текста с помощью произвольной функции шумоподавления и (2) обучения модели восстанавливать исходный текст.\n",
    "\n",
    "BART особенно эффективен при тонкой настройке для генерации текста (например, обобщение, перевод), но также хорошо работает для задач понимания (например, классификация текста, ответы на вопросы). Этот конкретный checkpoint была обучен на CNN Daily Mail, большого набора данных пар текст-резюме.\n",
    "\n",
    "**Назначение и ограничения**\n",
    "Вы можете использовать эту модель для суммирования текста."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b5693b8",
   "metadata": {},
   "source": [
    "**Evaluation results**\n",
    "\n",
    "eval_rouge1\t42.621\n",
    "\n",
    "eval_rouge2\t21.9825\n",
    "\n",
    "eval_rougeL\t33.034\n",
    "\n",
    "eval_rougeLsum\t39.6783\n",
    "\n",
    "test_rouge1\t41.3174\n",
    "\n",
    "test_rouge2\t20.8716\n",
    "\n",
    "test_rougeL\t32.1337\n",
    "\n",
    "test_rougeLsum\t38.4149"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe3fe4e4",
   "metadata": {},
   "source": [
    "Предположительно loss:\n",
    "\n",
    "**criterion label_smoothed_cross_entropy**\n",
    "\n",
    "https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.summarization.md#4-fine-tuning-on-cnn-dm-summarization-task\n",
    "4) Fine-tuning on CNN-DM summarization task:\n",
    "Example fine-tuning CNN-DM\n",
    "\n",
    "TOTAL_NUM_UPDATES=20000  \n",
    "WARMUP_UPDATES=500      \n",
    "LR=3e-05\n",
    "MAX_TOKENS=2048\n",
    "UPDATE_FREQ=4\n",
    "BART_PATH=/path/to/bart/model.pt\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 fairseq-train cnn_dm-bin \\\n",
    "    --restore-file $BART_PATH \\\n",
    "    --max-tokens $MAX_TOKENS \\\n",
    "    --task translation \\\n",
    "    --source-lang source --target-lang target \\\n",
    "    --truncate-source \\\n",
    "    --layernorm-embedding \\\n",
    "    --share-all-embeddings \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "    --required-batch-size-multiple 1 \\\n",
    "    --arch bart_large \\\n",
    "    --criterion label_smoothed_cross_entropy \\\n",
    "    --label-smoothing 0.1 \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 \\\n",
    "    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\n",
    "    --clip-norm 0.1 \\\n",
    "    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
    "    --fp16 --update-freq $UPDATE_FREQ \\\n",
    "    --skip-invalid-size-inputs-valid-test \\\n",
    "    --find-unused-parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db50304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "750d975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer_philschmid= pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "cc201ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'The residents of Brienz, in the eastern canton of Graubünden, Switzerland, were asked to leave their homes by Friday evening due to the risk of a rockslide. Two million cubic metres of loose rock is coming loose from the mountain and could obliterate the village. Even the dairy cows were loaded up for departure.'}]\n"
     ]
    }
   ],
   "source": [
    "for i in summarizer_philschmid(ARTICLE_bbc_news):\n",
    "    for k, v in i.items():\n",
    "        print(summarizer_philschmid(ARTICLE_bbc_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "6d771beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 258\n",
      "summary length: 54\n",
      "The residents of Brienz, in the eastern canton of Graubünden, Switzerland, were asked to leave their homes by Friday evening due to the risk of a rockslide. Two million cubic metres of loose rock is coming loose from the mountain and could obliterate the village. Even the dairy cows were loaded up for departure.\n",
      "{'rouge1': 0.38983050847457623, 'rouge2': 0.13793103448275862, 'rougeL': 0.22033898305084743, 'rougeLsum': 0.30508474576271183}\n"
     ]
    }
   ],
   "source": [
    "eval_article_summary('The residents of Brienz, in the eastern canton of Graubünden, Switzerland, were asked to leave their homes by Friday evening due to the risk of a rockslide. Two million cubic metres of loose rock is coming loose from the mountain and could obliterate the village. Even the dairy cows were loaded up for departure.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "5603dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'The book seems to have been written by a 7th grader with poor grammatical skills for her age. There is a misspelling on the cover and there are at least one per chapter. I was so distracted by the poor writing and weak plot that I decided to read with a pencil in hand to mark all of the horrible grammar and spelling.'}]\n"
     ]
    }
   ],
   "source": [
    "for i in summarizer_philschmid(REVIEW):\n",
    "    for k, v in i.items():\n",
    "        print(summarizer_philschmid(REVIEW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "5036c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 129\n",
      "summary length: 63\n",
      "The book seems to have been written by a 7th grader with poor grammatical skills for her age. There is a misspelling on the cover and there are at least one per chapter. I was so distracted by the poor writing and weak plot that I decided to read with a pencil in hand to mark all of the horrible grammar and spelling.\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_review_summary('The book seems to have been written by a 7th grader with poor grammatical skills for her age. There is a misspelling on the cover and there are at least one per chapter. I was so distracted by the poor writing and weak plot that I decided to read with a pencil in hand to mark all of the horrible grammar and spelling.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "9a077dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"Jane's friend Leila is thinking about joining the hospital. Doctor Rudra will help her with some questions about the administration. Leila will come to his chamber tomorrow to talk to him about it. He will be more than happy to help her.  \"}]\n"
     ]
    }
   ],
   "source": [
    "for i in summarizer_philschmid(DIALOGUE):\n",
    "    for k, v in i.items():\n",
    "        print(summarizer_philschmid(DIALOGUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "0050e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 44\n",
      "Jane's friend Leila is thinking about joining the hospital. Doctor Rudra will help her with some questions about the administration. Leila will come to his chamber tomorrow to talk to him about it. He will be more than happy to help her.  \n",
      "{'rouge1': 0.3333333333333333, 'rouge2': 0.06896551724137931, 'rougeL': 0.2, 'rougeLsum': 0.2}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary(\"Jane's friend Leila is thinking about joining the hospital. Doctor Rudra will help her with some questions about the administration. Leila will come to his chamber tomorrow to talk to him about it. He will be more than happy to help her.  \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "492ec30a",
   "metadata": {},
   "source": [
    "**Перепроверим результат на другом диалоге**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "dcdda846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"Rana will have the corn mushroom soup, grilled chicken with mashed potatoes and peas and garlic rice. Amal will have beef, with bread and salad, and a chocolate cold coffee. Waiter will take Rana's order and bring the order to Amal.\"}]\n"
     ]
    }
   ],
   "source": [
    "for i in summarizer_philschmid(conversation_test):\n",
    "    for k, v in i.items():\n",
    "        print(summarizer_philschmid(conversation_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "42b0f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 41\n",
      "Rana will have the corn mushroom soup, grilled chicken with mashed potatoes and peas and garlic rice. Amal will have beef, with bread and salad, and a chocolate cold coffee. Waiter will take Rana's order and bring the order to Amal.\n",
      "{'rouge1': 0.033898305084745756, 'rouge2': 0.0, 'rougeL': 0.033898305084745756, 'rougeLsum': 0.033898305084745756}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary(\"Rana will have the corn mushroom soup, grilled chicken with mashed potatoes and peas and garlic rice. Amal will have beef, with bread and salad, and a chocolate cold coffee. Waiter will take Rana's order and bring the order to Amal.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27d79f92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c40d3abe",
   "metadata": {},
   "source": [
    "# Google’s T5 Model from Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "90859a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4b503675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_mac/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abacc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b2d6f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "33b87f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(ARTICLE_bbc_news, return_tensors='pt', max_length=512, truncation=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398e4af4",
   "metadata": {},
   "source": [
    "In the above code we have used encode() method and passed following parameters -\n",
    "\n",
    "return_tensors: If set, will return tensors instead of a list of python integers. We can also use “tf” for TensorFlow, “pt” for PyTorch and “np” for Numpy.\n",
    "\n",
    "max_length: Controls the maximum length to use by one of the truncation/padding parameters.\n",
    "\n",
    "truncation: Activates and controls truncation.\n",
    "\n",
    "The max_length and truncation parameters together indicate that we do not want the original text to bypass 512 tokens, which is the default limit set for tokenization in Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ac2a1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs, \n",
    "                        min_length=40, \n",
    "                        max_length=200, \n",
    "                        length_penalty=1, \n",
    "                        num_beams=4, \n",
    "                        early_stopping=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "788b6ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> residents of Brienz, in the eastern canton of Graubünden, have all been evacuated because of the risk of an imminent rockslide. two million cubic metres of rock is coming loose from the mountain above, and a rockslide could obliterate the village.</s>']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.batch_decode(output)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "48270ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 258\n",
      "summary length: 41\n",
      "residents of Brienz, in the eastern canton of Graubünden, have all been evacuated because of the risk of an imminent rockslide. two million cubic metres of rock is coming loose from the mountain above, and a rockslide could obliterate the village.\n",
      "{'rouge1': 0.41904761904761906, 'rouge2': 0.17475728155339806, 'rougeL': 0.32380952380952377, 'rougeLsum': 0.34285714285714286}\n"
     ]
    }
   ],
   "source": [
    "eval_article_summary('residents of Brienz, in the eastern canton of Graubünden, have all been evacuated because of the risk of an imminent rockslide. two million cubic metres of rock is coming loose from the mountain above, and a rockslide could obliterate the village.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4eaa8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(REVIEW, return_tensors='pt', max_length=512, truncation=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "382db770",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs, \n",
    "                        min_length=10, \n",
    "                        max_length=30, \n",
    "                        length_penalty=1, \n",
    "                        num_beams=4, \n",
    "                        early_stopping=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "43d416ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><extra_id_0>I was so distracted by the poor writing and weak plot, that I decided to read with a pencil to mark all of the horrible grammar']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.batch_decode(output)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "be82cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 129\n",
      "summary length: 26\n",
      "I was so distracted by the poor writing and weak plot, that I decided to read with a pencil to mark all of the horrible grammar\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_review_summary('I was so distracted by the poor writing and weak plot, that I decided to read with a pencil to mark all of the horrible grammar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a46db08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(DIALOGUE, return_tensors='pt', max_length=512, truncation=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b8ca98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs, \n",
    "                        min_length=10, \n",
    "                        max_length=50, \n",
    "                        length_penalty=1, \n",
    "                        num_beams=4, \n",
    "                        early_stopping=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "6e1b0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><extra_id_0>I am doing well. And you? Doctor Rudra: Good morning, Jane. I am doing well. And you? Doctor Rudra: Good morning, Jane. I am doing well. And you? Doctor Rudra: Good morning']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.batch_decode(output)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "1a9c3f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 32\n",
      "I am doing well. And you? Doctor Rudra: Good morning, Jane. I am doing well. And you? Doctor Rudra: Good morning, Jane. I am doing well. And you? Doctor Rudra: Good morning\n",
      "{'rouge1': 0.12244897959183672, 'rouge2': 0.0425531914893617, 'rougeL': 0.12244897959183672, 'rougeLsum': 0.12244897959183672}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary(\"\"\"I am doing well. And you? Doctor Rudra: Good morning, Jane. I am doing well. And you? Doctor Rudra: Good morning, Jane. I am doing well. And you? Doctor Rudra: Good morning\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5587381",
   "metadata": {},
   "source": [
    "Краткое содержание диалога смысла не имеет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "293fbbe5",
   "metadata": {},
   "source": [
    "**Перепроверим результат на другом диалоге**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9601d9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><extra_id_0>Can I start you off with some refreshing drink? Rana: Yes. Amal: And I’ll have a chocolate cold coffee. Waiter: Hello, good evening. Can I take the order now? Rana: Yes']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(conversation_test, return_tensors='pt', max_length=512, truncation=True) \n",
    "output = model.generate(inputs, \n",
    "                        min_length=10, \n",
    "                        max_length=50, \n",
    "                        length_penalty=1, \n",
    "                        num_beams=4, \n",
    "                        early_stopping=True\n",
    "                        )\n",
    "sequences = tokenizer.batch_decode(output)\n",
    "sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe0491e9",
   "metadata": {},
   "source": [
    "По-прежнему, качество саммаризации оставляет желать лучшего. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04672d8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12dbffe2",
   "metadata": {},
   "source": [
    "# HF mT5 Model from Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "681a40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id, max_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "19a53d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Accidental rockslide'}]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(ARTICLE_bbc_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "6bc7a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 258\n",
      "summary length: 2\n",
      "Accidental rockslide\n",
      "{'rouge1': 0.03076923076923077, 'rouge2': 0.0, 'rougeL': 0.03076923076923077, 'rougeLsum': 0.03076923076923077}\n"
     ]
    }
   ],
   "source": [
    "eval_article_summary(\"Accidental rockslide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "27c92f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Poor grammar and spelling'}]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(REVIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "37180eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 129\n",
      "summary length: 4\n",
      "Poor grammar and spelling\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_review_summary('Poor grammar and spelling')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b95c410",
   "metadata": {},
   "source": [
    "Модель показывает достаточно интересные результаты.\n",
    "Проверим на большой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4323cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('summary predictions.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca2b9c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>model predicted summary</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16748</th>\n",
       "      <td>4.0</td>\n",
       "      <td>a &amp;quot;come all you weary strangers and a story i might tell&amp;quot;</td>\n",
       "      <td>Very boring</td>\n",
       "      <td>Not the most gripping novel of Burroughs it serves to introduce him very well. Here the most daring explorer of words of the twentieth century broke into the territory of the nineteenth century novel and its filosophical background without entirely playing by its rules of course. First reason to read this book is that it leads you to the far more challenging and swifter written second and third part of the trilogy to which it belongs. Although fragmented it is quite accessible in Burroughs terms, but therefore also a bit boring at times. The vastness of its scope, as if you are presented with a grand painting of Delacroix in which all that is of worth in this world is presented, is enticing enough. Central theme might be evolution taken to its extremes. And without much further ado it prophesies some diseases and other threats with which we fool ourselfs in becoming adjusted just now. Notice how accurate the descriptions of this writer can be, who has so many times been accused of drugblurred nonsense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14005</th>\n",
       "      <td>3.0</td>\n",
       "      <td>It's a good book</td>\n",
       "      <td>Very enjoyable</td>\n",
       "      <td>My sister really enjoys David Sedaris' books. I gave her this book for xmas. While not as laugh out loud funny as her fave book Naked, she told me she still recommends it to anyone who enjoys laughing at themselves and their family.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11122</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Not bad</td>\n",
       "      <td>Very interesting</td>\n",
       "      <td>As a non- english speaker backgorund person, this book reveals many secrets of the English language. However, I was expenting to see more examples about the gramma explained in the book. Too much theory makes it a bit bore to read it and understand it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>1.0</td>\n",
       "      <td>What Happened?</td>\n",
       "      <td>I've been a Christian all my life but I realized I had never read the</td>\n",
       "      <td>I've been a Christian all my life but I realized I had never read the bible, so I bought it and read it front to back and its the damnedest thing... now i'm an Atheist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13738</th>\n",
       "      <td>3.0</td>\n",
       "      <td>A pretty good Silhouette Romance</td>\n",
       "      <td>Very quick read</td>\n",
       "      <td>\"Her Secret Children\" is the first book that I read by Judith McWillians and the first Silhouette Romance that I have read in a very long while. In this book American Vicky Sutton learns that her frozen eggs were given to an English couple, the result is twins. Wanting to see her children, Vicky goes to England were she meets James Thayer, thier father.While I liked the plot I have admitt I wished that the story was given more time to develop. I know these are to be fairly quick reads, I would have loved to emotions from the characters.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score  \\\n",
       "16748    4.0   \n",
       "14005    3.0   \n",
       "11122    3.0   \n",
       "4525     1.0   \n",
       "13738    3.0   \n",
       "\n",
       "                                                                   summary  \\\n",
       "16748  a &quot;come all you weary strangers and a story i might tell&quot;   \n",
       "14005                                                     It's a good book   \n",
       "11122                                                              Not bad   \n",
       "4525                                                        What Happened?   \n",
       "13738                                     A pretty good Silhouette Romance   \n",
       "\n",
       "                                                     model predicted summary  \\\n",
       "16748                                                            Very boring   \n",
       "14005                                                         Very enjoyable   \n",
       "11122                                                       Very interesting   \n",
       "4525   I've been a Christian all my life but I realized I had never read the   \n",
       "13738                                                        Very quick read   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          review  \n",
       "16748  Not the most gripping novel of Burroughs it serves to introduce him very well. Here the most daring explorer of words of the twentieth century broke into the territory of the nineteenth century novel and its filosophical background without entirely playing by its rules of course. First reason to read this book is that it leads you to the far more challenging and swifter written second and third part of the trilogy to which it belongs. Although fragmented it is quite accessible in Burroughs terms, but therefore also a bit boring at times. The vastness of its scope, as if you are presented with a grand painting of Delacroix in which all that is of worth in this world is presented, is enticing enough. Central theme might be evolution taken to its extremes. And without much further ado it prophesies some diseases and other threats with which we fool ourselfs in becoming adjusted just now. Notice how accurate the descriptions of this writer can be, who has so many times been accused of drugblurred nonsense.  \n",
       "14005                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   My sister really enjoys David Sedaris' books. I gave her this book for xmas. While not as laugh out loud funny as her fave book Naked, she told me she still recommends it to anyone who enjoys laughing at themselves and their family.  \n",
       "11122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               As a non- english speaker backgorund person, this book reveals many secrets of the English language. However, I was expenting to see more examples about the gramma explained in the book. Too much theory makes it a bit bore to read it and understand it.  \n",
       "4525                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    I've been a Christian all my life but I realized I had never read the bible, so I bought it and read it front to back and its the damnedest thing... now i'm an Atheist.  \n",
       "13738                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \"Her Secret Children\" is the first book that I read by Judith McWillians and the first Silhouette Romance that I have read in a very long while. In this book American Vicky Sutton learns that her frozen eggs were given to an English couple, the result is twins. Wanting to see her children, Vicky goes to England were she meets James Thayer, thier father.While I liked the plot I have admitt I wished that the story was given more time to develop. I know these are to be fairly quick reads, I would have loved to emotions from the characters.  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdb6bdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>model predicted summary</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Would you like to know more?</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Let me add a piece of advice to the ideas proposed in this book. When you hear the words, &amp;quot;Would you like to know more?&amp;quot; just say &amp;quot;No thank you, and close the door. Closing the door is VERY important. When I lived in Salt Lake City, I had to learn to do that, because some people wouldn't take no for an answer. One furious Relief Society lady was so upset with my attitude of unbelief that she pointed to my dog and said, &amp;quot;I suppose you think that dog is just as good as you are!&amp;quot; I replied, &amp;quot;Probably better.&amp;quot; and THEN I shut the door and watched through the window as her embarrased husband tried to lead her down the porch stairs. She must have spread the word in the neighborhood, because I wasn't bothered after that. Though this book poses some good questions, my advice is to not even get started or you are in for an evening of flip charts!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                       summary model predicted summary  \\\n",
       "6632    2.0  Would you like to know more?               Very good   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review  \n",
       "6632  Let me add a piece of advice to the ideas proposed in this book. When you hear the words, &quot;Would you like to know more?&quot; just say &quot;No thank you, and close the door. Closing the door is VERY important. When I lived in Salt Lake City, I had to learn to do that, because some people wouldn't take no for an answer. One furious Relief Society lady was so upset with my attitude of unbelief that she pointed to my dog and said, &quot;I suppose you think that dog is just as good as you are!&quot; I replied, &quot;Probably better.&quot; and THEN I shut the door and watched through the window as her embarrased husband tried to lead her down the porch stairs. She must have spread the word in the neighborhood, because I wasn't bothered after that. Though this book poses some good questions, my advice is to not even get started or you are in for an evening of flip charts!  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index==6632]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c05ef1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>model predicted summary</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Forbush a false, carnal teacher about Jesus' life</td>\n",
       "      <td>Very interesting, but a very interesting read.</td>\n",
       "      <td>Forbush equates good Judaism and Christianity with patriotism, and his teachings contain carnal error and a pathetically low view of the Lord Jesus. Forbush has no understanding of Messiahship. For example, he thinks Jesus was a follower of John the Baptist and hero-worshipped John. Jesus, he says, joined John's \"movement\", having been baptized into it, making Jesus a disciple of John! Jesus bowed Himself at John's feet. From his 1912 children's Life of Jesus, Bible Study Union Series, here are examples of his doctrine:\"Jesus hastened from His hilltop home, down the Jordan valley, to meet John. Through all these years of silence He had been studying the same problems which John had faced in the desert, and He was, no doubt, now feeling very clearly that His own great future was just at hand. He must have listened to the impassioned speaker with admiration. He must have felt toward him something of hero-worship.\" (pages 44-45).\"[Jesus] bowed Himself at the feet of John as his follower and allied Himself with this movement for the purifying of the nation's life. When Jesus came and asked to be baptized by John He was not claiming leadership, He was becoming a soldier in the ranks...\" (page 46)Forbush was a teacher who unfortunately did not understand spiritual things. I am sorry to find his works still being published.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                                            summary  \\\n",
       "4522    1.0  Forbush a false, carnal teacher about Jesus' life   \n",
       "\n",
       "                             model predicted summary  \\\n",
       "4522  Very interesting, but a very interesting read.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          review  \n",
       "4522  Forbush equates good Judaism and Christianity with patriotism, and his teachings contain carnal error and a pathetically low view of the Lord Jesus. Forbush has no understanding of Messiahship. For example, he thinks Jesus was a follower of John the Baptist and hero-worshipped John. Jesus, he says, joined John's \"movement\", having been baptized into it, making Jesus a disciple of John! Jesus bowed Himself at John's feet. From his 1912 children's Life of Jesus, Bible Study Union Series, here are examples of his doctrine:\"Jesus hastened from His hilltop home, down the Jordan valley, to meet John. Through all these years of silence He had been studying the same problems which John had faced in the desert, and He was, no doubt, now feeling very clearly that His own great future was just at hand. He must have listened to the impassioned speaker with admiration. He must have felt toward him something of hero-worship.\" (pages 44-45).\"[Jesus] bowed Himself at the feet of John as his follower and allied Himself with this movement for the purifying of the nation's life. When Jesus came and asked to be baptized by John He was not claiming leadership, He was becoming a soldier in the ranks...\" (page 46)Forbush was a teacher who unfortunately did not understand spiritual things. I am sorry to find his works still being published.  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index==4522]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa2ba262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>model predicted summary</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>1.0</td>\n",
       "      <td>One of the worst books I've ever read</td>\n",
       "      <td>Very funny</td>\n",
       "      <td>I'm a French student of English, in my third year at university. I have to read \"Tess of the D'Urbervilles\" for my British literature class and it's just terrible. The author spends so much time describing Tess' beauty, virtue, kindness and courage, that I wonder why her name is not Mary Sue. There is no character development : instead, we are told a hundred times that Tess' eye colour is somewhere between blue, violet, grey and black -- seriously, can you picture that ?Furthermore, the heroine's so-called \"kindness\" is nothing but stupid weakness in my opinion. Her love interest, Angel Clare -- who, for some obscure reason, is also loved by most of her dairymaid friends --, blames and rejects her for being raped, and yet she still worships him and behaves like his obedient slave, despite his cruelty to her. This is merely a ridiculous and unhealthy obsession, not a true love story.To my mind, the only likeable character is Alec D'Urberville. Although he is supposed to be the \"villain\", I find him rather funny and much less annoying than Tess and Angel. Too bad that Thomas Hardy did not choose Alec as the main character. The novel would have been more enjoyable.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                                summary model predicted summary  \\\n",
       "2479    1.0  One of the worst books I've ever read              Very funny   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            review  \n",
       "2479  I'm a French student of English, in my third year at university. I have to read \"Tess of the D'Urbervilles\" for my British literature class and it's just terrible. The author spends so much time describing Tess' beauty, virtue, kindness and courage, that I wonder why her name is not Mary Sue. There is no character development : instead, we are told a hundred times that Tess' eye colour is somewhere between blue, violet, grey and black -- seriously, can you picture that ?Furthermore, the heroine's so-called \"kindness\" is nothing but stupid weakness in my opinion. Her love interest, Angel Clare -- who, for some obscure reason, is also loved by most of her dairymaid friends --, blames and rejects her for being raped, and yet she still worships him and behaves like his obedient slave, despite his cruelty to her. This is merely a ridiculous and unhealthy obsession, not a true love story.To my mind, the only likeable character is Alec D'Urberville. Although he is supposed to be the \"villain\", I find him rather funny and much less annoying than Tess and Angel. Too bad that Thomas Hardy did not choose Alec as the main character. The novel would have been more enjoyable.  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index==2479]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "451269d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>model predicted summary</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Would you like to know more?</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Let me add a piece of advice to the ideas proposed in this book. When you hear the words, &amp;quot;Would you like to know more?&amp;quot; just say &amp;quot;No thank you, and close the door. Closing the door is VERY important. When I lived in Salt Lake City, I had to learn to do that, because some people wouldn't take no for an answer. One furious Relief Society lady was so upset with my attitude of unbelief that she pointed to my dog and said, &amp;quot;I suppose you think that dog is just as good as you are!&amp;quot; I replied, &amp;quot;Probably better.&amp;quot; and THEN I shut the door and watched through the window as her embarrased husband tried to lead her down the porch stairs. She must have spread the word in the neighborhood, because I wasn't bothered after that. Though this book poses some good questions, my advice is to not even get started or you are in for an evening of flip charts!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                       summary model predicted summary  \\\n",
       "6632    2.0  Would you like to know more?               Very good   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review  \n",
       "6632  Let me add a piece of advice to the ideas proposed in this book. When you hear the words, &quot;Would you like to know more?&quot; just say &quot;No thank you, and close the door. Closing the door is VERY important. When I lived in Salt Lake City, I had to learn to do that, because some people wouldn't take no for an answer. One furious Relief Society lady was so upset with my attitude of unbelief that she pointed to my dog and said, &quot;I suppose you think that dog is just as good as you are!&quot; I replied, &quot;Probably better.&quot; and THEN I shut the door and watched through the window as her embarrased husband tried to lead her down the porch stairs. She must have spread the word in the neighborhood, because I wasn't bothered after that. Though this book poses some good questions, my advice is to not even get started or you are in for an evening of flip charts!  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index==6632]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed299339",
   "metadata": {},
   "source": [
    "Можно видеть, что некоторые результаты достаточно противоречивы (краткое содержание id 4522, 2014, 6632 явно не совпадает с кратким содержание автора и противоречит поставленной оценке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d3797dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Good, Jane'}]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(DIALOGUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "15dd1175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 2\n",
      "Good, Jane\n",
      "{'rouge1': 0.10526315789473684, 'rouge2': 0.0, 'rougeL': 0.10526315789473684, 'rougeLsum': 0.10526315789473684}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary('Good, Jane')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0bc5279",
   "metadata": {},
   "source": [
    "Краткое содержание диалога смысла не имеет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3031b06f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c20cba2",
   "metadata": {},
   "source": [
    "# bert-extractive-summarizer GPT2 based Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "396493bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer,TransformerSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "287622d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilgpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0b4fdb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residents of a tiny Swiss village have all been evacuated because of the risk of an imminent rockslide. Even the dairy cows were loaded up for departure after geologists warned a rockfall was imminent. Two million cubic metres of rock is coming loose from the mountain above, and a rockslide could obliterate the village.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_mac/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(GPT2_model(ARTICLE_bbc_news, min_length=60))\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6d958668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 258\n",
      "summary length: 54\n",
      "Residents of a tiny Swiss village have all been evacuated because of the risk of an imminent rockslide. Even the dairy cows were loaded up for departure after geologists warned a rockfall was imminent. Two million cubic metres of rock is coming loose from the mountain above, and a rockslide could obliterate the village.\n",
      "{'rouge1': 0.4615384615384615, 'rouge2': 0.24347826086956523, 'rougeL': 0.37606837606837606, 'rougeLsum': 0.37606837606837606}\n"
     ]
    }
   ],
   "source": [
    "eval_article_summary('Residents of a tiny Swiss village have all been evacuated because of the risk of an imminent rockslide. Even the dairy cows were loaded up for departure after geologists warned a rockfall was imminent. Two million cubic metres of rock is coming loose from the mountain above, and a rockslide could obliterate the village.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c4b30788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will not put much faith in the reviews from now on!\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(GPT2_model(REVIEW, \n",
    "                          # min_length=5, \n",
    "                          max_length=60\n",
    "                        ))\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "a05d9517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 129\n",
      "summary length: 12\n",
      "I will not put much faith in the reviews from now on!\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_review_summary('I will not put much faith in the reviews from now on!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "fbd8daa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well.\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(GPT2_model(DIALOGUE, min_length=9, max_length=20))\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "7225a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 4\n",
      "I am doing well.\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary('I am doing well.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2436ab31",
   "metadata": {},
   "source": [
    "Краткое содержание диалога смысла не имеет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1579d9b7",
   "metadata": {},
   "source": [
    "**Перепроверим результат на другом диалоге**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "30573d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rana: Yes. Waiter: Ok.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_mac/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(GPT2_model(conversation_test, min_length=9, max_length=20))\n",
    "print(full)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27712510",
   "metadata": {},
   "source": [
    "Краткое содержание диалога по-прежему смысла не имеет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eff6c7b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fc041ec",
   "metadata": {},
   "source": [
    "# bert-extractive-summarizer XLNet based Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "bbc3ca9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "136ca357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brienz, in the eastern canton of Graubünden, is now empty.\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(model(ARTICLE_bbc_news, min_length=40, max_length=70))\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "ca4eee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 258\n",
      "summary length: 10\n",
      "Brienz, in the eastern canton of Graubünden, is now empty.\n",
      "{'rouge1': 0.08108108108108109, 'rouge2': 0.0, 'rougeL': 0.08108108108108109, 'rougeLsum': 0.08108108108108109}\n"
     ]
    }
   ],
   "source": [
    "eval_article_summary('Brienz, in the eastern canton of Graubünden, is now empty.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "86279f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel I have to write to keep others from wasting their money. This book seems to have been written by a 7th grader with poor grammatical skills for her age!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_mac/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(model(REVIEW, \n",
    "                     min_length=3,\n",
    "                    #  max_length=5\n",
    "                     ))\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "eb58393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 129\n",
      "summary length: 31\n",
      "I feel I have to write to keep others from wasting their money. This book seems to have been written by a 7th grader with poor grammatical skills for her age!\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_review_summary('I feel I have to write to keep others from wasting their money. This book seems to have been written by a 7th grader with poor grammatical skills for her age!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "235bdeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane: Good morning, Doctor Rudra, how are you doing? It’s a pleasure to meet you. Leila: It’s a pleasure to meet you, Doctor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_mac/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(model(DIALOGUE, min_length=10))\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "d6d38c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 23\n",
      "Jane: Good morning, Doctor Rudra, how are you doing? It’s a pleasure to meet you. Leila: It’s a pleasure to meet you, Doctor.\n",
      "{'rouge1': 0.2380952380952381, 'rouge2': 0.05, 'rougeL': 0.2380952380952381, 'rougeLsum': 0.2380952380952381}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary('Jane: Good morning, Doctor Rudra, how are you doing? It’s a pleasure to meet you. Leila: It’s a pleasure to meet you, Doctor.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "289880fa",
   "metadata": {},
   "source": [
    "Краткое содержание диалога смысла не имеет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6a2745a",
   "metadata": {},
   "source": [
    "**Перепроверим результат на другом диалоге**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0f8b0257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiter: Hello, good evening. Amal: And I’ll have a chocolate cold coffee. Rana: No no we are ready, you can take the order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_mac/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "full = ''.join(model(conversation_test, min_length=10))\n",
    "print(full)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e63b31ed",
   "metadata": {},
   "source": [
    "Краткое содержание диалога по-прежему смысла не имеет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca1443c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0eba1a26",
   "metadata": {},
   "source": [
    "# chart-gpt4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35461d01",
   "metadata": {},
   "source": [
    "На сайте https://chat-gpt.ru/ попросила предоставить резюме по текстам, обозначенным в начале блокнота. Длину краткого содержания не ограничивала. Ниже результаты:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "384d0dd9",
   "metadata": {},
   "source": [
    "**по новостной статье:**\n",
    "\n",
    "Residents of the Swiss village Brienz have been evacuated due to risk of an imminent rock slide. The village was built on land that is subsiding down towards the valley, causing the church spire to lean and large cracks to appear in buildings. Scientists have warned that two million cubic metres of loosened rock could fall down the mountainside onto the village due to heavy rain. Switzerland's Alpine regions are particularly sensitive to global warming, as the permafrost high in the mountains begins to thaw, causing the rock to become unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "9b74939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_gpt4_predict = \"\"\"Residents of the Swiss village Brienz have been evacuated due to risk of an imminent rock slide. The village was built on land that is subsiding down towards the valley, causing the church spire to lean and large cracks to appear in buildings. Scientists have warned that two million cubic metres of loosened rock could fall down the mountainside onto the village due to heavy rain. Switzerland's Alpine regions are particularly sensitive to global warming, as the permafrost high in the mountains begins to thaw, causing the rock to become unstable.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "55d597d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 258\n",
      "summary length: 91\n",
      "Residents of the Swiss village Brienz have been evacuated due to risk of an imminent rock slide. The village was built on land that is subsiding down towards the valley, causing the church spire to lean and large cracks to appear in buildings. Scientists have warned that two million cubic metres of loosened rock could fall down the mountainside onto the village due to heavy rain. Switzerland's Alpine regions are particularly sensitive to global warming, as the permafrost high in the mountains begins to thaw, causing the rock to become unstable.\n",
      "{'rouge1': 0.3483870967741936, 'rouge2': 0.14379084967320263, 'rougeL': 0.30967741935483867, 'rougeLsum': 0.30967741935483867}\n"
     ]
    }
   ],
   "source": [
    "eval_article_summary(chart_gpt4_predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "666b56ff",
   "metadata": {},
   "source": [
    "Результат с ограничением по длине до 50 слов.\n",
    "\n",
    "Residents of a small Swiss village have been evacuated due to the risk of a rockslide, giving them just 48 hours to leave their homes. Brienz, located in the Graubünden region, is at risk due to subsiding land causing cracks. The village is now empty, with even dairy cows being taken to safety. Global warming is also said to be contributing to the risk in Alpine regions, with melting permafrost making rocks more unstable. Two million cubic metres of rock is at risk of falling onto the village, with residents waiting in temporary accommodation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dec290f",
   "metadata": {},
   "source": [
    "**по отзыву:**\n",
    "\n",
    "Book poorly written, don't buy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "eee9e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 129\n",
      "summary length: 5\n",
      "Book poorly written, don't buy.\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_review_summary(\"Book poorly written, don't buy.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47649121",
   "metadata": {},
   "source": [
    "Ниже результат с ограничением по длине до 5 слов.\n",
    "\n",
    "Book poorly written with weak plot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b50befdd",
   "metadata": {},
   "source": [
    "**по диалогу:**\n",
    "\n",
    "Jane greets Doctor Rudra and introduces her friend Leila who is interested in joining the hospital. Jane asks Doctor Rudra if he can provide information about the hospital administration. Doctor Rudra agrees and invites Leila to meet him in his chamber the following day. Leila thanks Doctor Rudra for his help and Doctor Rudra hopes to provide helpful information to her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4e991bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 107\n",
      "summary length: 61\n",
      "Jane greets Doctor Rudra and introduces her friend Leila who is interested in joining the hospital. Jane asks Doctor Rudra if he can provide information about the hospital administration. Doctor Rudra agrees and invites Leila to meet him in his chamber the following day. Leila thanks Doctor Rudra for his help and Doctor Rudra hopes to provide helpful information to her.\n",
      "{'rouge1': 0.2820512820512821, 'rouge2': 0.10526315789473685, 'rougeL': 0.23076923076923075, 'rougeLsum': 0.23076923076923075}\n"
     ]
    }
   ],
   "source": [
    "eval_dialogue_summary('Jane greets Doctor Rudra and introduces her friend Leila who is interested in joining the hospital. Jane asks Doctor Rudra if he can provide information about the hospital administration. Doctor Rudra agrees and invites Leila to meet him in his chamber the following day. Leila thanks Doctor Rudra for his help and Doctor Rudra hopes to provide helpful information to her.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae00b118",
   "metadata": {},
   "source": [
    "Two customers order drinks and dinner at a restaurant, with one requesting iced tea and corn mushroom soup, and the other requesting a chocolate cold coffee and beef."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7213c7ce",
   "metadata": {},
   "source": [
    "Ниже результат по запросу с ограничением по длине до 40 слов.\n",
    "\n",
    "Jane greets Doctor Rudra and introduces her friend Leila, who is interested in joining the hospital. Doctor Rudra offers to speak with Leila about the administration and invites her to visit his chamber the next day. Leila thanks him and hopes he can help her."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de689563",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
